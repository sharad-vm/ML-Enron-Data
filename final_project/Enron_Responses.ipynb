{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enron Submission Free-Response Questions\n",
    "##### Sharad Mohan Vijalapuram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”]**\n",
    "\n",
    "Enron Corporation, an American energy company based in Houston, TX was formed by Kenneth Lay in 1985 and by the year 2000, it was one of the largest companies in the US. It is believed that Jeffrey Skilling was one of the key persons in developing a staff of executives that used accounting loopholes and poor financial reporting to hide billions of dollars in debt from failed deals and projects. Towards the end of 2001, the company collpsed into bankruptcy.\n",
    "\n",
    "The Enron scandal, publicized in October 2001, eventually led to the bankruptcy of the company, whcih turned out to be the largest bankruptcy reorganization in American history at that time and the biggest audit failure.\n",
    "\n",
    "The Enron dataset used in this project consists of financial and email data of 146 executives of which 18 were POI. Each data point (executive) has 21 features. The goal of this project is to use different machine learning algorithms and potentially tune them to accurately identify if a person is of interest or not. As part of the project, I used different features of the data set and created a few new features when necessary to try and improve the accuracy, precision and recall of various different algorithms to finally pick one algorithm that has the best of the scores. \n",
    "\n",
    "###### Data Overview\n",
    "\n",
    "I used Python to load the dataset into a dataframe and explred the dataset for some basic statistics and structure.\n",
    "\n",
    "* Number of executives: 146\n",
    "* Number of POIs: 18\n",
    "* Number of features: 21\n",
    "\n",
    "###### Outliers\n",
    "\n",
    "I also looked for any potential outliers and came up with the below, all of which I removed from the dataset prior to training and testing a few algorithms.\n",
    "\n",
    "* I plotted Salary against Bonus to find one data point that was way off compared to the rest and after filtering on the dataframe, I realized that it was just a 'Total' of all the datapoints. Clearly, this will not help us in any predictions and so I dropped from the dataframe\n",
    "* I looked for executives who have no valid values in any of their features and found LOCKHART EUGENE E to have nothing populated and since this will not help in anyway and might even skew the error, I dropped this record\n",
    "* I also found a record that looked less like an executive and more like an agency. Since we are looking to identify a Person Of Interest(POI) and not an agency, I dropped this record from the dataframe\n",
    "* Interestingly, I found 2 executives whose features seems to have been shifted to right. I updated the features of the 2 executives - Belfer Robert, Bhatnagar Sanjay before proceeding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  [relevant rubric items: “create new features”, “intelligently select features”, “properly scale features”]**\n",
    "\n",
    "##### Features used:\n",
    "\n",
    "I started with all the features available except email address to see how algorithms perform and then tried adding new features to see if they had any effect on the algorthms. \n",
    "\n",
    "Here is how the algorithms performed initially:\n",
    "\n",
    "| Classifier            | Precision | Recall  | F1 Score | Accuracy |\n",
    "|-----------------------|-----------|---------|----------|----------|\n",
    "| RandomForest          | 0.55      | 0.17    | 0.27     | 0.87     |\n",
    "| GaussianNB            | 0.23      | 0.37    | 0.30     | 0.76     |\n",
    "| DecisionTree          | 0.33      | 0.34    | 0.33     | 0.82     |\n",
    "| KNeighbors            | 0.35      | 0.05    | 0.09     | 0.86     |\n",
    "| AdaBoost              | 0.48      | 0.35    | 0.40     | 0.86     |\n",
    "| ExtraTrees            | 0.50      | 0.17    | 0.26     | 0.86     |\n",
    "| KMeans (n_clusters=2) | 0.21      | 0.08    | 0.12     | 0.83     |\n",
    "\n",
    "Although I have some good numbers for accuracy, my recall and precision could still improve. \n",
    "\n",
    "Creating and adding fraction_from_poi, fraction_to_poi and fraction_shared_receipt to the features list, provided the following results: \n",
    "\n",
    "| Classifier            | Precision | Recall  | F1 Score | Accuracy |\n",
    "|-----------------------|-----------|---------|----------|----------|\n",
    "| RandomForest          | 0.67      | 0.27    | 0.36     | 0.88     |\n",
    "| GaussianNB            | 0.25      | 0.37    | 0.30     | 0.76     |\n",
    "| DecisionTree          | 0.61      | 0.61    | 0.61     | 0.89     |\n",
    "| KNeighbors            | 0.35      | 0.05    | 0.09     | 0.86     |\n",
    "| AdaBoost              | 0.65      | 0.53    | 0.58     | 0.89     |\n",
    "| ExtraTrees            | 0.64      | 0.27    | 0.38     | 0.88     |\n",
    "| KMeans (n_clusters=2) | 0.21      | 0.08    | 0.12     | 0.83     |\n",
    "\n",
    "This boosted the F1 score on almost all the models and also made 2 algorithms stand out - Decision Tree and AdaBoost.\n",
    "\n",
    "I further added bonus_to_salary and bonus_to_total to the list and here is how the models performed with this addition:\n",
    "\n",
    "| Classifier            | Precision | Recall  | F1 Score | Accuracy |\n",
    "|-----------------------|-----------|---------|----------|----------|\n",
    "| RandomForest          | 0.62      | 0.23    | 0.34     | 0.87     |\n",
    "| GaussianNB            | 0.25      | 0.37    | 0.30     | 0.76     |\n",
    "| DecisionTree          | 0.62      | 0.60    | 0.61     | 0.89     |\n",
    "| KNeighbors            | 0.35      | 0.05    | 0.09     | 0.86     |\n",
    "| AdaBoost              | 0.62      | 0.51    | 0.51     | 0.89     |\n",
    "| ExtraTrees            | 0.63      | 0.25    | 0.36     | 0.88     |\n",
    "| KMeans (n_clusters=2) | 0.21      | 0.08    | 0.12     | 0.83     |\n",
    "\n",
    "The 2 new features did not have any quantitative effect on most of the models and even degraded the overall performance of a few models and so I had to revert this change back by dropping these newly created features.\n",
    "\n",
    "For my final design, I used SelectKBest to get the best features to obtain the the best precision and recall scores during testing. I created train and test sets with the use of Stratified Shuffle Split cross validation due to small size of the data and that of the POI list. \n",
    "\n",
    "The final features utilized in the model and their imoportance are:\n",
    "\n",
    "* fraction_to_poi : 25.8782\n",
    "* total_stock_value : 22.5105\n",
    "* exercised_stock_options : 22.3490\n",
    "* bonus : 20.7923\n",
    "* salary : 18.2897\n",
    "* fraction_shared_receipt : 15.6936\n",
    "* deferred_income : 11.4249\n",
    "* shared_receipt_with_poi : 10.4091\n",
    "* long_term_incentive : 9.9222\n",
    "* total_payments : 9.2839\n",
    "* restricted_stock : 8.8254\n",
    "* loan_advances : 7.1841\n",
    "* from_poi_to_this_person : 5.4787\n",
    "* expenses : 5.4189\n",
    "* other : 4.2024\n",
    "* fraction_from_poi : 2.5928\n",
    "* from_this_person_to_poi : 2.4456\n",
    "\n",
    "##### Feature creation:\n",
    "\n",
    "As explained above, I created five new features using some of the existing features. I created fraction_to_poi and fraction_from_poi features, to see if perhaps I could identify a POI by whether that person had a certain percentage of his/her outgoing or incoming emails with a POI. I also created fraction_shared_receipt, bonus_to_salary and bonus_to_total because an executive's high expense/salary ratio or bonus to salary or total income ratios could be potentially flagged as POIs. However, only 3 of these 5 features made it through the best features. \n",
    "\n",
    "This was evident when I added the 2 bonus to salary and total ratios to the features list and tested for performance. Adding these features to the feature list resulted in a poorer performance and so I dropped these features from the final list before sending it to SelectKBest model.\n",
    "\n",
    "##### Feature scaling:\n",
    "\n",
    "I did not have to do any feature scaling, because I used the AdaBoost classifier, which uses boosted decision trees. Decision trees do not need feature scaling. I did implement a feature scaling function when I was testing out classifier types though -- in particular I needed it for the KMeans classifier because that classifier doesn't function well when data from different features are scaled fairly differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?  [relevant rubric item: “pick an algorithm”]**\n",
    "\n",
    "I tried out seven different algorithms: Random Forest, Decision Tree, Naive Bayes, KMeans, Nearest Neighbors, Ada Boost and Extra Trees. I used the algorithm defaults and different feature list as you can observe from the python notebook.\n",
    "\n",
    "I ended up using a tuned Adaboost classifier with a decision tree base estimator as that resulted in the highest accuracy as well as F1 score.\n",
    "\n",
    "F1 score is nothing but a weighted average of precision and recall. An F1 score is considered to be the best at 1 and worst at 0. Precision and Recall can equally affect the F1 score and since the goal of this project is also to have a precision and recall above 0.3, I've used F1 score instead. \n",
    "\n",
    "The formula for F1 score is: \\\\[\\frac{2\\ (precision\\ *\\ recall)}{precision + recall} \\\\]\n",
    "\n",
    "The performance of different models varied a lot between algorithms. Most of them had lower precision or recall than the accuracy. This is probably due to the low volume of data. Details on how each of the 7 algorithms performed along with their scores are available in the project here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? What parameters did you tune? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).  [relevant rubric items: “discuss parameter tuning”, “tune the algorithm”]**\n",
    "\n",
    "Parameter Tuning is the process of altering the parameters fed to an algorithm to manage or potentially improve performance. Tuning does not necessarily improve the performance and efficiency all the time. Poor tuning can result in decreased performance or effciency of the chosen algorithm. \n",
    "\n",
    "I used GridSearchCV for my DecisionTree and AdaBoost classifiers with a range of parameter values that I picked that made sense. I used the built in parameter, scoring to result in a classifier that is designed to produce the highest F1 score. I also used selectKBest to get the best features from the whole list and used those features towards my final steps. The combination of using these two tuning methods resulting in the best scores and accuracies of my algorithms.\n",
    "\n",
    "My final model was Decision Tree classifier and I used select K best to get the 17 best features and used them along with the best parameters suggested by GridSearchCV out performed any other algorithms or tuning methods during my testing. Below if my final parameter setting.\n",
    "\n",
    "{DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=20,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "            splitter='best'))])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?  [relevant rubric items: “discuss validation”, “validation strategy”]**\n",
    "\n",
    "Validation in Machine Learning is the process of checking how well the model or algorithm would fit on data outside of the training data set used in training the model. This is usually achieved by splitting the avaialble data set into two chunks - one for training and one for testing. The most commonly used ratio for this split is 70:30. The model is trained on the split 70% of the data and validated by testing on the 30% of the data. \n",
    "\n",
    "A classic mistake is to 'overfit' the data by closely modeling the algorithm based on the enitre data set that would result in the best performance on the same data but would fail on any data outside of the training data. \n",
    "\n",
    "To validate my model, I used the tester.py script provided that uses the Stratified Shuffle Split cross-validation method that provides metrics on the classifer. Stratified Shuffle Split is used because with this cross validation method, the data is shuffled every time and then split which means the test sets may overlap between the splits, although this is not guaranteed. Since our dataset is small and the number of POI is even smaller, Stratified Shuffle Split is ideal in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]**\n",
    "\n",
    "My final model had the following average performance metrics:\n",
    "\n",
    "**Accuracy: 0.92**\n",
    "This means my model was 92% accurate in predicting whether a person was a POI or not. That is, my model would predict an executive as a POI or not correctly 92 times on 100. It is very important to get the accuracy as high as possible in order to make the model reliable but, it is not the only factor to be considered in analyzinf how good or bad a model is. \n",
    "\n",
    "**Precision: 0.68**\n",
    "This means that 68% of the people my model classified as POIs were actually POIs.  That is, my model would not result in false accusations 68 times on 100. So, the ratio of true_POIs/(false_POIs + true_POIs) was 0.68. This is important because we don't falsely accuse people of being POIs -- so the higher our precision score, the lower our percentage of false accusations.\n",
    "\n",
    "**Recall: 0.78**\t\n",
    "This means that 78% of the POIs in the data were correctly identified by the model. That is, my model would correctly predict 78 times on 100 POIs. In mathematical terms, this is the ratio of true_POIs/(false_non_POIs + true_POIs). This is important because we want to catch as many of the POIs as possible, to make sure they face justice. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
